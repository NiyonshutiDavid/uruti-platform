# Uruti RL Platform - Presentation Agent Training System

A comprehensive reinforcement learning framework for training agents to optimize presentation delivery through slide navigation and engagement optimization.

---

## ğŸ¯ Quick Start

### 1. Create Sample Presentation

```bash
python3 -c "from utils.slide_manager import SlideManager; \
SlideManager().create_sample_presentation('sample_presentation')"
```

### 2. Train Agent (Choose Algorithm)

```bash
# Train DQN  
python3 train_presentation.py --algorithm dqn --timesteps 50000

# Train PPO with custom config
python3 train_presentation.py --algorithm ppo --config configs/ppo_v1.json --timesteps 100000

# Train A2C
python3 train_presentation.py --algorithm a2c --timesteps 50000
```

### 3. Run Trained Agent

```bash
python3 presentation_demo.py \
    --model_path models/dqn_presentation_slides10 \
    --algorithm dqn \
    --episodes 1
```

### 4. Compare Results

```bash
python3 compare_models.py
```

---

## ğŸ“ Project Structure

```
uruti_rl/
â”‚
â”œâ”€â”€ ğŸ“‚ Core Training Scripts
â”‚   â”œâ”€â”€ train.py                    # Main training script (existing)
â”‚   â”œâ”€â”€ train_presentation.py       # â­ NEW: Presentation-focused training
â”‚   â”œâ”€â”€ evaluate.py                 # Model evaluation
â”‚   â”œâ”€â”€ compare_models.py           # Algorithm comparison
â”‚   â””â”€â”€ environment_demo.py         # Environment demos
â”‚
â”œâ”€â”€ ğŸ“‚ environments/ (envs/)
â”‚   â”œâ”€â”€ pitch_env.py               # Simulated pitch environment
â”‚   â”œâ”€â”€ video_pitch_env.py         # MELD video-based environment
â”‚   â””â”€â”€ presentation_pitch_env.py  # â­ NEW: Slide-based presentation
â”‚
â”œâ”€â”€ ğŸ“‚ utilities/ (utils/)
â”‚   â”œâ”€â”€ slide_manager.py           # â­ NEW: Slide management
â”‚   â””â”€â”€ presentation_comparison.py # â­ NEW: Performance analysis
â”‚
â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ feature_extractor.py
â”‚
â”œâ”€â”€ ğŸ“‚ configurations/ (configs/)
â”‚   â”œâ”€â”€ dqn_*.json                 # 10 DQN variant configs
â”‚   â”œâ”€â”€ ppo_*.json                 # 10 PPO variant configs
â”‚   â”œâ”€â”€ a2c_*.json                 # 10 A2C variant configs
â”‚   â”œâ”€â”€ reinforce_*.json           # 10 REINFORCE configs
â”‚   â””â”€â”€ generate_hyperparameter_variants.py
â”‚
â”œâ”€â”€ ğŸ“‚ models/ (trained)
â”‚   â””â”€â”€ [trained agent files]
â”‚
â”œâ”€â”€ ğŸ“‚ reports/
â”‚   â”œâ”€â”€ comparison.md              # Algorithm comparison results
â”‚   â”œâ”€â”€ comparison.json
â”‚   â””â”€â”€ [evaluation reports]
â”‚
â”œâ”€â”€ ğŸ“„ README.md                   # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ sample_presentation/           # Sample slides directory
```

---

## ğŸ¤– Presentation Pitch Environment

### Overview

The **PresentationPitchEnv** (`envs/presentation_pitch_env.py`) trains agents to:
- âœ… Navigate through presentation slides
- âœ… Maintain audience engagement
- âœ… Complete presentations within time constraints
- âœ… Optimize delivery through multiple actions

### Agent Actions

| ID | Action | Effect | Base Reward |
|----|---------| --------|------------|
| 0  | Maintain | Hold current slide | +0.05 |
| 1  | Increase Energy | Boost confidence & engagement | +0.40 |
| 2  | Use Gestures | Improve engagement & clarity | +0.30 |
| 3  | Eye Contact | Strong engagement boost | +0.45 |
| 4  | **Next Slide** | **Advance to next slide** â­ | **+1.20** |
| 5  | Storytelling | Maximum engagement boost | +0.60 |

### Observation Space (6-dimension)

```python
[confidence, engagement, clarity, pace, slide_progress, time_remaining]
```

- **confidence** (0-1): Presenter confidence level
- **engagement** (0-1): Audience engagement
- **clarity** (0-1): Message clarity
- **pace** (0-2): Presentation pace
- **slide_progress** (0-1): Slides completed
- **time_remaining** (0-1): Time left (0 = finished)

### Key Learning Objective

Agents learn **when to advance to the next slide** (Action 4) to maximize:
- Total reward (higher is better)
- Slides completed (more is better)
- Audience engagement (maintain > 0.7)

---

## ğŸ§  Supported Algorithms

| Algorithm | Status | Config Files | Features |
|-----------|--------|-------------|----------|
| **DQN** | âœ… Supported | 10 variants | Exploration-Exploitation |
| **PPO** | âœ… Supported | 10 variants | Policy Gradient |
| **A2C** | âœ… Supported | 10 variants | Advantage Actor-Critic |
| **REINFORCE** | âœ… Supported | 10 variants | Policy Gradient |

---

## ğŸš€ Training Commands

### Basic Training

```bash
# Quick training (5-10 minutes)
python3 train_presentation.py --algorithm dqn --timesteps 10000

# Standard training (30+ minutes)
python3 train_presentation.py --algorithm ppo --timesteps 100000

# Training with custom config
python3 train_presentation.py \
    --algorithm a2c \
    --config configs/a2c_v1.json \
    --timesteps 100000

# Train all algorithms
for algo in dqn ppo a2c reinforce; do
    echo "Training $algo..."
    python3 train_presentation.py --algorithm $algo --timesteps 50000 &
done
wait
```

### Training with Custom Slides

```bash
# Create slide directory
mkdir my_presentation
cp /path/to/slides/*.png my_presentation/

# Train with custom slides
python3 train_presentation.py \
    --algorithm ppo \
    --slides_dir my_presentation \
    --timesteps 100000
```

### Environment Options

```bash
python3 train_presentation.py \
    --algorithm dqn \
    --env_type presentation  # or 'simulation'
    --slides 10              # number of slides
    --timesteps 50000
```

---

## ğŸ® Running Trained Agents

### Demo Modes

```bash
# Single episode
python3 presentation_demo.py \
    --model_path models/dqn_presentation_slides10 \
    --algorithm dqn \
    --episodes 1

# Multiple episodes with statistics
python3 presentation_demo.py \
    --model_path models/ppo_presentation_slides10 \
    --algorithm ppo \
    --episodes 5 \
    --deterministic

# With custom slides
python3 presentation_demo.py \
    --model_path models/dqn_presentation \
    --algorithm dqn \
    --slides_dir my_presentation \
    --episodes 3
```

---

## ğŸ“Š Analysis & Comparison

### Compare Algorithms

```bash
# Compare on presentation task
python3 -c "
from utils.presentation_comparison import PresentationComparator
c = PresentationComparator()
results = c.compare_algorithms(['dqn', 'ppo', 'a2c'], episodes=10)
report = c.generate_comparison_report(results)
print(report)
"
```

### View Comparison Results

```bash
cat comparison.md
```

### Benchmark Hyperparameter Variants

```bash
# Compare all variants of one algorithm
python3 -c "
from utils.presentation_comparison import PresentationComparator
c = PresentationComparator()
variants = c.benchmark_hyperparameters('ppo', variants=10)
report = c.generate_comparison_report(variants)
print(report)
"
```

---

## ğŸ“š Key Files Reference

### Training & Evaluation

| File | Purpose | Run |
|------|---------|-----|
| `train_presentation.py` | Train agents on presentation | `python3 train_presentation.py --algorithm dqn` |
| `presentation_demo.py` | Run trained agent demo | `python3 presentation_demo.py --model_path ...` |
| `train.py` | Original training script | `python3 train.py --algorithm ppo` |
| `compare_models.py` | Compare algorithms | `python3 compare_models.py` |
| `evaluate.py` | Detailed evaluation | `python3 evaluate.py --model_path ...` |

### Environment Implementations

| File | Type | Features |
|------|------|----------|
| `envs/presentation_pitch_env.py` | ğŸ†• Presentation | Slide navigation, real slides, Pygame UI |
| `envs/pitch_env.py` | ğŸ“Š Simulation | Simulated metrics, basic UI |
| `envs/video_pitch_env.py` | ğŸ¥ Video | MELD dataset, MediaPipe poses |

### Utilities

| File | Purpose |
|------|---------|
| `utils/slide_manager.py` | Slide loading, presets, sample generation |
| `utils/presentation_comparison.py` | Performance analysis, report generation |
| `hyperparameter_tuner.py` | Batch training with configurations |
| `visualizer.py` | Pygame-based visualization |
| `report_generator.py` | Markdown reports |
| `pdf_report_generator.py` | PDF report generation |

### Configurations

| File | Purpose | Count |
|------|---------|-------|
| `configs/dqn_*.json` | DQN hyperparameters | 10 variants |
| `configs/ppo_*.json` | PPO hyperparameters | 10 variants |
| `configs/a2c_*.json` | A2C hyperparameters | 10 variants |
| `configs/reinforce_*.json` | REINFORCE hyperparameters | 10 variants |

---

## ğŸ¨ Presentation Features

### Real-Time Visualization

During training/demo, see live:
- **Slide Display** - Current presentation slide
- **Metrics Gauges** - Confidence, Engagement, Clarity
- **Progress Bars** - Slide progress, time remaining
- **Status Panel** - Current action, step count, tips

### Slide Management

```python
from utils.slide_manager import SlideManager, PresentationConfig

# Create sample presentation
manager = SlideManager(total_slides=10)
manager.create_sample_presentation('slides')

# Or use presets
from utils.slide_manager import STANDARD_PITCH, INVESTOR_PITCH

# Standard: 10 slides, 30 seconds
# Investor: 12 slides, 40 seconds with custom timings
```

### Supported Slide Formats

- PNG
- JPG
- BMP
- GIF

---

## ğŸ“ˆ Expected Performance

### Good Training Results

After training, well-performing agents should:
- âœ… **Complete 80%+ of slides**
- âœ… **Maintain engagement > 0.7**
- âœ… **Episode reward > 50**
- âœ… **Balance speed and quality**

### Performance Metrics

| Metric | Meaning | Good Range |
|--------|---------|-----------|
| Avg Reward | Total episode reward | > 50 |
| Slides Completed | Number of slides shown | > 8/10 |
| Final Engagement | Ending engagement level | > 0.7 |
| Time Used | Percentage of time used | 85-100% |

---

## ğŸ”§ Installation & Setup

### Requirements

```bash
# Install dependencies
pip install -r requirements.txt
```

### Verify Installation

```bash
python3 -c "
from envs.presentation_pitch_env import PresentationPitchEnv
from utils.slide_manager import SlideManager
print('âœ… Presentation environment ready!')
"
```

---

## ğŸ› Troubleshooting

### Training Issues

| Problem | Solution |
|---------|----------|
| **No slides showing** | Create sample: `python3 -c "from utils.slide_manager import SlideManager; SlideManager().create_sample_presentation()"` |
| **Agent not learning** | Increase timesteps or adjust environment rewards |
| **Out of memory** | Reduce slides count or use simulation mode |
| **Model not found** | Check model path: `ls models/` |

### Running Issues

| Problem | Solution |
|---------|----------|
| **Import errors** | Install requirements: `pip install -r requirements.txt` |
| **Module not found** | Ensure working directory is `uruti_rl/` |
| **Pygame error** | On macOS: `brew install python-tk` |

---

## ğŸ“Š Performance Tracking

### View Training Logs

```bash
# Tensorboard visualization
tensorboard --logdir models/logs

# Or check results
ls -lh models/eval_logs/
```

### Save Results

```bash
# Comparison results already saved
cat comparison.md

# Generate new report
python3 -c "
from utils.presentation_comparison import PresentationComparator
c = PresentationComparator()
r = c.compare_algorithms(['dqn', 'ppo', 'a2c'])
report = c.generate_comparison_report(r, output_file='my_report.md')
"
```

---

## ğŸ¯ Advanced Usage

### Train Multiple Variants

```bash
# Train all 40 hyperparameter configs on presentation environment
bash -c 'for config in configs/dqn_v*.json; do
    python3 train_presentation.py --algorithm dqn --config "$config" \
        --env_type presentation --timesteps 50000 &
done; wait'
```

### Compare Across Environments

```bash
# Compare on both simulation and presentation
python3 -c "
from utils.presentation_comparison import PresentationComparator
c = PresentationComparator()
dqn_sim = c.compare_environments('dqn', env_types=['simulation'])
dqn_pres = c.compare_environments('dqn', env_types=['presentation'])
"
```

### Custom Presentation Types

```python
from utils.slide_manager import PresentationConfig

# Define custom presentation
custom = PresentationConfig(
    total_slides=15,
    total_duration=60.0,
    slides_dir='my_slides',
    slide_timing={
        0: 15.0,   # Title slide - long
        1: 8.0,    # Problem  
        # ... custom timing per slide
    }
)
```

---

## ğŸ“ File Statistics

- **Python Code**: 1,581 lines
- **Configuration Files**: 44 JSON variants
- **Documentation**: Comprehensive README + examples
- **Total Components**: 70 files

---

## ğŸ“ Learning Resources

### Understanding the System

1. **Basic Training** â†’ Start with `train_presentation.py`
2. **Agent Design** â†’ See `envs/presentation_pitch_env.py`
3. **Analysis** â†’ Use `utils/presentation_comparison.py`
4. **Advanced** â†’ Modify configs and algorithms

### Example Workflows

#### Workflow 1: Quick Test (5 min)
```bash
python3 -c "from utils.slide_manager import SlideManager; \
SlideManager().create_sample_presentation()"
python3 train_presentation.py --algorithm dqn --timesteps 5000
python3 presentation_demo.py --model_path models/dqn_presentation_slides10 \
    --algorithm dqn --episodes 1
```

#### Workflow 2: Full Training (1 hour)
```bash
# Train all algorithms
for algo in dqn ppo a2c; do
    python3 train_presentation.py --algorithm $algo --timesteps 100000 &
done
wait

# Compare results
python3 compare_models.py
```

#### Workflow 3: Custom Domain (2+ hours)
```bash
mkdir my_domain_slides
cp /path/to/domain/slides/*.png my_domain_slides/

for config in configs/ppo_v*.json; do
    python3 train_presentation.py --algorithm ppo --config "$config" \
        --slides_dir my_domain_slides --timesteps 100000 &
done
wait
```

---

## ğŸš€ Next Steps

1. **Create slides** â†’ `mkdir slides && cp your_slides/*.png slides/`
2. **Train model** â†’ `python3 train_presentation.py --algorithm ppo`
3. **Run demo** â†’ `python3 presentation_demo.py --model_path models/ppo_presentation_slides10 --algorithm ppo`
4. **Analyze** â†’ `python3 compare_models.py && cat comparison.md`

---

## ğŸ“„ Project Overview

This project provides a complete RL framework for:
- âœ… Training agents on presentation delivery
- âœ… Learning optimal slide navigation  
- âœ… Maintaining audience engagement
- âœ… Comparing algorithm performance
- âœ… Analyzing presentation strategies

### Architecture Highlights

- **Modular Design** - Easy to extend and customize
- **Multiple Environments** - Simulation, MELD video, and presentation modes
- **40 Hyperparameter Variants** - Systematic parameter tuning
- **4 RL Algorithms** - DQN, PPO, A2C, REINFORCE
- **Beautiful Visualization** - Real-time Pygame UI
- **Comprehensive Analysis** - Automated comparison and reporting

---

## ğŸ“ Support

For issues or questions:
1. Check `comparison.md` for performance data
2. Review source code comments in `envs/presentation_pitch_env.py`
3. Check utility documentation in `utils/`
4. Verify installation with example scripts

---

**Ready to train your presentation agent!** ğŸ¯

Last Updated: February 2025  
Version: 2.0 - Presentation Environment Integration
